# -*- coding: utf-8 -*-
"""Streamlit News Alert App

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14TSVVtE6n0AbtaonjUp0-7AeqT5-OpjZ
"""

import streamlit as st
import pandas as pd
from newspaper import Article, Config, build
import nltk
import ssl

# --- C·∫•u h√¨nh v√† Thi·∫øt l·∫≠p ban ƒë·∫ßu ---

# C·∫•u h√¨nh ƒë·ªÉ b·ªè qua x√°c th·ª±c SSL (h·ªØu √≠ch khi m·ªôt s·ªë trang web c√≥ ch·ª©ng ch·ªâ c≈©)
# ƒêi·ªÅu n√†y gi√∫p tr√°nh l·ªói khi t·∫£i d·ªØ li·ªáu t·ª´ m·ªôt s·ªë trang tin
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# T·∫£i t√†i nguy√™n c·∫ßn thi·∫øt cho th∆∞ vi·ªán newspaper
# Ch·ªâ c·∫ßn ch·∫°y m·ªôt l·∫ßn
try:
    nltk.data.find('tokenizers/punkt')
except nltk.downloader.DownloadError:
    nltk.download('punkt')

# C·∫•u h√¨nh cho newspaper3k ƒë·ªÉ tƒÉng t·ªëc v√† tr√°nh l·ªói
user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'
config = Config()
config.browser_user_agent = user_agent
config.request_timeout = 10 # Th·ªùi gian ch·ªù t·ªëi ƒëa cho m·ªói y√™u c·∫ßu l√† 10 gi√¢y

# --- H√†m x·ª≠ l√Ω ch√≠nh ---

@st.cache_data(ttl=3600) # L∆∞u k·∫øt qu·∫£ trong 1 gi·ªù (3600 gi√¢y) ƒë·ªÉ kh√¥ng ph·∫£i qu√©t l·∫°i li√™n t·ª•c
def load_data(file_path):
    """H√†m ƒë·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ file CSV v√† x·ª≠ l√Ω c∆° b·∫£n."""
    try:
        # B·ªè qua 2 d√≤ng ƒë·∫ßu ti√™n kh√¥ng c·∫ßn thi·∫øt trong file CSV c·ªßa b·∫°n
        df = pd.read_csv(file_path, skiprows=2)
        # ƒê·ªïi t√™n c·ªôt ƒë·ªÉ d·ªÖ s·ª≠ d·ª•ng
        df.columns = ['STT', 'Website', 'Keywords']
        # Lo·∫°i b·ªè c√°c d√≤ng kh√¥ng c√≥ th√¥ng tin Website ho·∫∑c Keywords
        df.dropna(subset=['Website', 'Keywords'], inplace=True)
        return df
    except FileNotFoundError:
        st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file '{file_path}'. Vui l√≤ng ƒë·∫£m b·∫£o file n√†y t·ªìn t·∫°i trong kho GitHub c·ªßa b·∫°n.")
        return None

def clean_keywords(keywords_str):
    """H√†m ƒë·ªÉ l√†m s·∫°ch v√† t√°ch c√°c t·ª´ kh√≥a."""
    # T√°ch chu·ªói b·∫±ng d·∫•u / ho·∫∑c xu·ªëng d√≤ng, lo·∫°i b·ªè k√Ω t·ª± '-' v√† kho·∫£ng tr·∫Øng th·ª´a
    keywords = [kw.strip().lstrip('-').strip() for kw in keywords_str.replace('/', '\n').split('\n')]
    # Lo·∫°i b·ªè c√°c chu·ªói r·ªóng
    return [kw for kw in keywords if kw]

@st.cache_data(ttl=3600)
def find_news_on_site(site_url, keywords):
    """
    H√†m qu√©t m·ªôt trang web c·ª• th·ªÉ ƒë·ªÉ t√¨m c√°c b√†i b√°o ch·ª©a t·ª´ kh√≥a.
    _site_url: URL c·ªßa trang web c·∫ßn qu√©t.
    keywords: Danh s√°ch c√°c t·ª´ kh√≥a c·∫ßn t√¨m.
    """
    found_articles = []
    try:
        # S·ª≠ d·ª•ng newspaper ƒë·ªÉ x√¢y d·ª±ng m·ªôt ngu·ªìn tin t·ª©c t·ª´ URL
        st.write(f"ƒêang qu√©t trang: {site_url}...")
        paper = build(site_url, config=config, memoize_articles=False)

        st.write(f"T√¨m th·∫•y {len(paper.articles)} b√†i vi·∫øt tr√™n trang ch·ªß. B·∫Øt ƒë·∫ßu ph√¢n t√≠ch...")

        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng b√†i vi·∫øt c·∫ßn ph√¢n t√≠ch ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô
        articles_to_check = paper.articles[:20]

        for article in articles_to_check:
            try:
                article.download()
                article.parse()

                # Ki·ªÉm tra t·ª´ kh√≥a trong ti√™u ƒë·ªÅ v√† n·ªôi dung b√†i vi·∫øt
                # Chuy·ªÉn t·∫•t c·∫£ v·ªÅ ch·ªØ th∆∞·ªùng ƒë·ªÉ t√¨m ki·∫øm kh√¥ng ph√¢n bi·ªát hoa/th∆∞·ªùng
                article_text = (article.title + ' ' + article.text).lower()

                for keyword in keywords:
                    if keyword.lower() in article_text:
                        # N·∫øu t√¨m th·∫•y, l∆∞u l·∫°i th√¥ng tin v√† tho√°t kh·ªèi v√≤ng l·∫∑p t·ª´ kh√≥a
                        found_articles.append({
                            'title': article.title,
                            'url': article.url,
                            'keyword': keyword
                        })
                        break # Chuy·ªÉn sang b√†i b√°o ti·∫øp theo khi ƒë√£ t√¨m th·∫•y m·ªôt t·ª´ kh√≥a
            except Exception as e:
                # B·ªè qua n·∫øu c√≥ l·ªói khi t·∫£i m·ªôt b√†i b√°o c·ª• th·ªÉ
                # st.warning(f"Kh√¥ng th·ªÉ t·∫£i b√†i vi·∫øt {article.url}. L·ªói: {e}")
                continue
    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ truy c·∫≠p ho·∫∑c ph√¢n t√≠ch trang {site_url}. L·ªói: {e}")

    return found_articles

# --- Giao di·ªán ng∆∞·ªùi d√πng Streamlit ---

st.set_page_config(page_title="Tr√¨nh Qu√©t Tin T·ª©c", layout="wide")

st.title("üì∞ B·∫£ng Tin T·ª©c Theo T·ª´ Kh√≥a")
st.markdown("·ª®ng d·ª•ng n√†y t·ª± ƒë·ªông qu√©t c√°c trang tin t·ª©c d·ª±a tr√™n danh s√°ch website v√† t·ª´ kh√≥a b·∫°n cung c·∫•p.")

# T·∫£i d·ªØ li·ªáu
data = load_data('news_data.csv')

if data is not None:
    st.sidebar.header("T√πy ch·ªçn")

    # Cho ph√©p ng∆∞·ªùi d√πng ch·ªçn c√°c trang web ƒë·ªÉ qu√©t
    all_sites = data['Website'].unique()
    selected_sites = st.sidebar.multiselect("Ch·ªçn trang web ƒë·ªÉ qu√©t:", options=all_sites, default=all_sites)

    if st.sidebar.button("üöÄ B·∫Øt ƒë·∫ßu qu√©t tin t·ª©c"):
        if not selected_sites:
            st.warning("Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt trang web ƒë·ªÉ qu√©t.")
        else:
            st.info(f"B·∫Øt ƒë·∫ßu qu√° tr√¨nh qu√©t cho {len(selected_sites)} trang web ƒë√£ ch·ªçn. Vui l√≤ng ch·ªù...")

            # L·ªçc dataframe d·ª±a tr√™n l·ª±a ch·ªçn c·ªßa ng∆∞·ªùi d√πng
            sites_to_scan = data[data['Website'].isin(selected_sites)]

            total_found = 0

            # L·∫∑p qua t·ª´ng trang web trong danh s√°ch ƒë√£ ch·ªçn
            for index, row in sites_to_scan.iterrows():
                site_url = row['Website']
                keywords_str = row['Keywords']

                # L√†m s·∫°ch v√† chu·∫©n b·ªã danh s√°ch t·ª´ kh√≥a
                keywords = clean_keywords(keywords_str)

                if not keywords:
                    continue

                st.markdown("---")
                st.subheader(f"üîç K·∫øt qu·∫£ t·ª´: {site_url}")

                with st.spinner(f"ƒêang t√¨m ki·∫øm tr√™n {site_url}..."):
                    found_news = find_news_on_site(site_url, keywords)

                if found_news:
                    total_found += len(found_news)
                    for news_item in found_news:
                        st.markdown(f"**- [{news_item['title']}]({news_item['url']})**")
                        st.caption(f"T√¨m th·∫•y v·ªõi t·ª´ kh√≥a: `{news_item['keyword']}`")
                else:
                    st.write("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o ph√π h·ª£p v·ªõi t·ª´ kh√≥a.")

            st.success(f"Ho√†n th√†nh! T·ªïng c·ªông t√¨m th·∫•y {total_found} tin t·ª©c ph√π h·ª£p.")

    st.markdown("---")
    st.header("Danh s√°ch Website v√† T·ª´ kh√≥a")
    st.dataframe(data)